{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from array import array\n",
    "import struct, os, random, math, pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH = './data'\n",
    "INPUT_LAYER_SIZE = 28 * 28\n",
    "HIDDEN_LAYER_SIZE = 16\n",
    "OUTPUT_LAYER_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(images_filepath, labels_filepath):\n",
    "    # Read labels\n",
    "    with open(labels_filepath, 'rb') as file:\n",
    "        magic, size = struct.unpack(\">II\", file.read(8))\n",
    "        if magic != 2049:\n",
    "            raise ValueError(f'Magic number mismatch, expected 2049, got {magic}')\n",
    "        labels = array(\"B\", file.read())\n",
    "    \n",
    "    # Read images\n",
    "    with open(images_filepath, 'rb') as file:\n",
    "        magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "        if magic != 2051:\n",
    "            raise ValueError(f'Magic number mismatch, expected 2051, got {magic}')\n",
    "        image_data = array(\"B\", file.read())\n",
    "    \n",
    "    # Process images\n",
    "    images = []\n",
    "    for i in range(size):\n",
    "        start = i * rows * cols\n",
    "        end = (i + 1) * rows * cols\n",
    "        image = [\n",
    "            [pixel / 255.0 for pixel in image_data[start + j * cols : start + (j + 1) * cols]]\n",
    "            for j in range(rows)\n",
    "        ]\n",
    "        images.append([pixel for row in image for pixel in row])\n",
    "    \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_images_filepath = os.path.join(INPUT_PATH, 'train-images')\n",
    "training_labels_filepath = os.path.join(INPUT_PATH, 'train-labels')\n",
    "test_images_filepath = os.path.join(INPUT_PATH, 'test-images')\n",
    "test_labels_filepath = os.path.join(INPUT_PATH, 'test-labels')\n",
    "\n",
    "TRAINING_IMAGES, TRAINING_LABELS = load_and_preprocess_data(training_images_filepath, training_labels_filepath)\n",
    "TESTING_IMAGES, TESTING_LABELS = load_and_preprocess_data(test_images_filepath, test_labels_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logic functions\n",
    "def relu(x):\n",
    "    return max(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return 1 * (x > 0)\n",
    "\n",
    "def softmax(x):\n",
    "    max_x = max(x)\n",
    "    exp_x = [math.exp(xi - max_x) for xi in x]\n",
    "    sum_exp_x = sum(exp_x)\n",
    "    softmax_x = [xi / sum_exp_x for xi in exp_x]\n",
    "    return softmax_x\n",
    "\n",
    "def compute_neuron_values(layer1_size, layer2_size, layer1, weights, biases):\n",
    "    layer2 = []\n",
    "    for i in range(layer2_size): # 16\n",
    "        weighted_sum = 0\n",
    "        for j in range(layer1_size): # 784\n",
    "            weighted_sum += layer1[j] * weights[i][j]\n",
    "        weighted_sum += biases[i]\n",
    "        if layer2_size == OUTPUT_LAYER_SIZE:\n",
    "            layer2.append(weighted_sum)\n",
    "        else:\n",
    "            layer2.append(relu(weighted_sum))\n",
    "    return layer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack():\n",
    "    with open('nn/weights_biases.pkl', 'rb') as f:\n",
    "        weights_biases_g = pickle.load(f)\n",
    "\n",
    "    weights_input_to_hidden1 = weights_biases_g['weights_input_to_hidden1']\n",
    "    biases_hidden1 = weights_biases_g['biases_hidden1']\n",
    "    weights_hidden1_to_hidden2 = weights_biases_g['weights_hidden1_to_hidden2']\n",
    "    biases_hidden2 = weights_biases_g['biases_hidden2']\n",
    "    weights_hidden2_to_output = weights_biases_g['weights_hidden2_to_output']\n",
    "    biases_output = weights_biases_g['biases_output']\n",
    "\n",
    "    weights = [\n",
    "       weights_input_to_hidden1, \n",
    "       weights_hidden1_to_hidden2, \n",
    "       weights_hidden2_to_output\n",
    "    ]\n",
    "\n",
    "    biases = [\n",
    "        biases_hidden1, \n",
    "        biases_hidden2, \n",
    "        biases_output\n",
    "    ]\n",
    "\n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(input_layer, weights, biases):\n",
    "    #unpack weights and biases as we need them for a forward pass\n",
    "\n",
    "    weights_input_to_hidden1 = weights[0]\n",
    "    weights_hidden1_to_hidden2 = weights[1]\n",
    "    weights_hidden2_to_output = weights[2]\n",
    "\n",
    "    biases_hidden1 = biases[0]\n",
    "    biases_hidden2 = biases[1]\n",
    "    biases_output = biases[2]\n",
    "\n",
    "    #calculate activations for each layer\n",
    "\n",
    "    hidden_layer_1 = compute_neuron_values(INPUT_LAYER_SIZE,\n",
    "                                        HIDDEN_LAYER_SIZE,\n",
    "                                        input_layer,\n",
    "                                        weights_input_to_hidden1,\n",
    "                                        biases_hidden1)\n",
    "\n",
    "\n",
    "    hidden_layer_2 = compute_neuron_values(HIDDEN_LAYER_SIZE,\n",
    "                                        HIDDEN_LAYER_SIZE,\n",
    "                                        hidden_layer_1,\n",
    "                                        weights_hidden1_to_hidden2,\n",
    "                                        biases_hidden2)\n",
    "\n",
    "\n",
    "    output_layer = compute_neuron_values(HIDDEN_LAYER_SIZE,\n",
    "                                        OUTPUT_LAYER_SIZE,\n",
    "                                        hidden_layer_2,\n",
    "                                        weights_hidden2_to_output,\n",
    "                                        biases_output)\n",
    "    \n",
    "    return [hidden_layer_1, hidden_layer_2, output_layer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_neural_network(test_images, test_labels, weights, biases):\n",
    "    num_correct = 0\n",
    "    num_test_images = len(test_images)\n",
    "    \n",
    "    for i in range(num_test_images):\n",
    "        image = test_images[i]\n",
    "        true_label = test_labels[i]\n",
    "        \n",
    "        output = forward_pass(image, weights, biases)[2]\n",
    "        predicted_label = output.index(max(output))\n",
    "        \n",
    "        # Compare with true label\n",
    "        if predicted_label == true_label:\n",
    "            num_correct += 1\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = num_correct / num_test_images * 100\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(training_images_set, training_labels_set, weights, biases, learning_rate=0.001):\n",
    "    #pure SGD: for every image, update weights and biases\n",
    "\n",
    "    weights_input_to_hidden1 = weights[0]\n",
    "    weights_hidden1_to_hidden2 = weights[1]\n",
    "    weights_hidden2_to_output = weights[2]\n",
    "\n",
    "    biases_hidden1 = biases[0]\n",
    "    biases_hidden2 = biases[1]\n",
    "    biases_output = biases[2]\n",
    "    \n",
    "    for i in range(len(training_images_set)):\n",
    "\n",
    "        new_weights = [\n",
    "            weights_input_to_hidden1, \n",
    "            weights_hidden1_to_hidden2, \n",
    "            weights_hidden2_to_output\n",
    "        ]\n",
    "\n",
    "        new_biases = [\n",
    "            biases_hidden1, \n",
    "            biases_hidden2, \n",
    "            biases_output\n",
    "        ]\n",
    "\n",
    "        if i % 10000 == 0 and i != 0:\n",
    "            print(\"accuracy at image \", i, \": \", test_neural_network(TESTING_IMAGES, TESTING_LABELS, new_weights, new_biases))\n",
    "    \n",
    "        input_layer = training_images_set[i]\n",
    "        label = training_labels_set[i]\n",
    "        neuron_activations = forward_pass(input_layer, new_weights, new_biases)\n",
    "\n",
    "        hidden_layer_1 = neuron_activations[0]\n",
    "        hidden_layer_2 = neuron_activations[1]\n",
    "        output_layer = softmax(neuron_activations[2])\n",
    "        \n",
    "        actual_output = [0] * len(output_layer)\n",
    "        actual_output[label] = 1\n",
    "\n",
    "        #output deltas\n",
    "        output_layer_deltas = [output_layer[i] - actual_output[i] for i in range(len(output_layer))]\n",
    "\n",
    "        # layer 2 deltas ----------------\n",
    "        hidden_layer_2_deltas = []\n",
    "        for i in range(len(hidden_layer_2)): # len 16\n",
    "            delta = 0\n",
    "            for j in range(len(output_layer)): # len 10\n",
    "                delta += weights_hidden2_to_output[j][i] * output_layer_deltas[j]\n",
    "            delta *= relu_derivative(hidden_layer_2[i])\n",
    "            hidden_layer_2_deltas.append(delta)\n",
    "        \n",
    "\n",
    "        #hidden layer 1 deltas ----------------\n",
    "        hidden_layer_1_deltas = []\n",
    "\n",
    "        for i in range(len(hidden_layer_1)):\n",
    "            delta = 0\n",
    "            for j in range(len(hidden_layer_2)):\n",
    "                delta += weights_hidden1_to_hidden2[j][i] * hidden_layer_2_deltas[j]\n",
    "            delta *= relu_derivative(hidden_layer_1[i])\n",
    "            hidden_layer_1_deltas.append(delta)\n",
    "        \n",
    "\n",
    "        #now update the weights and biases\n",
    "        for i in range(len(hidden_layer_2)): #len 16\n",
    "            for j in range(len(output_layer)): # len 10\n",
    "                weights_hidden2_to_output[j][i] -= learning_rate * output_layer_deltas[j] * hidden_layer_2[i]\n",
    "            \n",
    "        for i in range(len(hidden_layer_1)):\n",
    "            for j in range(len(hidden_layer_2)):\n",
    "                weights_hidden1_to_hidden2[j][i] -= learning_rate * hidden_layer_2_deltas[j] * hidden_layer_1[i]\n",
    "\n",
    "        for i in range(len(input_layer)):\n",
    "            for j in range(len(hidden_layer_1)):\n",
    "                weights_input_to_hidden1[j][i] -= learning_rate * hidden_layer_1_deltas[j] * input_layer[i]\n",
    "\n",
    "        for i in range(len(biases_output)):\n",
    "            biases_output[i] -= learning_rate * output_layer_deltas[i]\n",
    "\n",
    "        for i in range(len(biases_hidden2)):\n",
    "            biases_hidden2[i] -= learning_rate * hidden_layer_2_deltas[i]\n",
    "\n",
    "        for i in range(len(biases_hidden1)):\n",
    "            biases_hidden1[i] -= learning_rate * hidden_layer_1_deltas[i]\n",
    "\n",
    "\n",
    "    weights_biases = {\n",
    "        'weights_input_to_hidden1': weights_input_to_hidden1,\n",
    "        'biases_hidden1': biases_hidden1,\n",
    "        'weights_hidden1_to_hidden2': weights_hidden1_to_hidden2,\n",
    "        'biases_hidden2': biases_hidden2,\n",
    "        'weights_hidden2_to_output': weights_hidden2_to_output,\n",
    "        'biases_output': biases_output\n",
    "    }\n",
    "\n",
    "    with open('nn/weights_biases.pkl', 'wb') as f:\n",
    "        pickle.dump(weights_biases, f)\n",
    "\n",
    "\n",
    "    print(\"SGD Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD_minibatch(training_images_set, training_labels_set, weights, biases, learning_rate=0.001, minibatch_size=32, batches=10000):\n",
    "    # average gradients for minibatchand apply in backpass\n",
    "\n",
    "\n",
    "    weights_input_to_hidden1 = weights[0]\n",
    "    weights_hidden1_to_hidden2 = weights[1]\n",
    "    weights_hidden2_to_output = weights[2]\n",
    "\n",
    "    biases_hidden1 = biases[0]\n",
    "    biases_hidden2 = biases[1]\n",
    "    biases_output = biases[2]\n",
    "\n",
    "    for batch in range(batches):\n",
    "        minibatch_indices = random.sample(range(len(training_images_set)), minibatch_size)\n",
    "        new_weights = [weights_input_to_hidden1, weights_hidden1_to_hidden2, weights_hidden2_to_output]\n",
    "        new_biases = [biases_hidden1, biases_hidden2, biases_output]\n",
    "\n",
    "        if batch % 1000 == 0 and batch != 0:\n",
    "            print(\"Accuracy after\", batch, \"minibatches:\", test_neural_network(TESTING_IMAGES, TESTING_LABELS, new_weights, new_biases))\n",
    "\n",
    "        # Initialize accumulators for gradients and deltas\n",
    "        final_output_deltas = [0] * OUTPUT_LAYER_SIZE\n",
    "        final_hidden2_deltas = [0] * HIDDEN_LAYER_SIZE\n",
    "        final_hidden1_deltas = [0] * HIDDEN_LAYER_SIZE\n",
    "\n",
    "        gradients_hidden2_to_output = [[0 for _ in range(HIDDEN_LAYER_SIZE)] for _ in range(OUTPUT_LAYER_SIZE)]\n",
    "        gradients_hidden1_to_hidden2 = [[0 for _ in range(HIDDEN_LAYER_SIZE)] for _ in range(HIDDEN_LAYER_SIZE)]\n",
    "        gradients_input_to_hidden1 = [[0 for _ in range(INPUT_LAYER_SIZE)] for _ in range(HIDDEN_LAYER_SIZE)]\n",
    "\n",
    "        for i in minibatch_indices:\n",
    "            input_layer = training_images_set[i]\n",
    "            label = training_labels_set[i]\n",
    "            neuron_activations = forward_pass(input_layer, new_weights, new_biases)\n",
    "\n",
    "            hidden_layer_1 = neuron_activations[0]\n",
    "            hidden_layer_2 = neuron_activations[1]\n",
    "            output_layer = softmax(neuron_activations[2])\n",
    "\n",
    "            actual_output = [0] * len(output_layer)\n",
    "            actual_output[label] = 1\n",
    "\n",
    "            output_layer_deltas = [output_layer[i] - actual_output[i] for i in range(len(output_layer))]\n",
    "            \n",
    "            #hidden2 to output gradients\n",
    "            for j in range(len(output_layer)):\n",
    "                for i in range(len(hidden_layer_2)):\n",
    "                    gradient = output_layer_deltas[j] * hidden_layer_2[i]\n",
    "                    gradients_hidden2_to_output[j][i] += gradient\n",
    "\n",
    "\n",
    "            #hidden1 to hidden2 output gradients\n",
    "            hidden_layer_2_deltas = [0] * HIDDEN_LAYER_SIZE\n",
    "            for j in range(len(hidden_layer_2)):\n",
    "                delta = sum(weights_hidden2_to_output[k][j] * output_layer_deltas[k] for k in range(OUTPUT_LAYER_SIZE))\n",
    "                delta *= relu_derivative(hidden_layer_2[j])\n",
    "                hidden_layer_2_deltas[j] = delta\n",
    "\n",
    "            for i in range(len(hidden_layer_1)):\n",
    "                for j in range(len(hidden_layer_2)):\n",
    "                    gradient = hidden_layer_2_deltas[j] * hidden_layer_1[i]\n",
    "                    gradients_hidden1_to_hidden2[j][i] += gradient\n",
    "\n",
    "            #input to hidden1 gradients\n",
    "            hidden_layer_1_deltas = [0] * HIDDEN_LAYER_SIZE\n",
    "            for j in range(len(hidden_layer_1)):\n",
    "                delta = sum(weights_hidden1_to_hidden2[k][j] * hidden_layer_2_deltas[k] for k in range(HIDDEN_LAYER_SIZE))\n",
    "                delta *= relu_derivative(hidden_layer_1[j])\n",
    "                hidden_layer_1_deltas[j] = delta\n",
    "\n",
    "            for i in range(len(input_layer)):\n",
    "                for j in range(len(hidden_layer_1)):\n",
    "                    gradient = hidden_layer_1_deltas[j] * input_layer[i]\n",
    "                    gradients_input_to_hidden1[j][i] += gradient\n",
    "\n",
    "            for i in range(OUTPUT_LAYER_SIZE):\n",
    "                final_output_deltas[i] += output_layer_deltas[i]\n",
    "\n",
    "            for i in range(HIDDEN_LAYER_SIZE):\n",
    "                final_hidden2_deltas[i] += hidden_layer_2_deltas[i]\n",
    "\n",
    "            for i in range(HIDDEN_LAYER_SIZE):\n",
    "                final_hidden1_deltas[i] += hidden_layer_1_deltas[i]\n",
    "\n",
    "        # Update weights and biases with averaged gradients\n",
    "\n",
    "        for i in range(HIDDEN_LAYER_SIZE):\n",
    "            for j in range(OUTPUT_LAYER_SIZE):\n",
    "                weights_hidden2_to_output[j][i] -= learning_rate * gradients_hidden2_to_output[j][i] / minibatch_size\n",
    "\n",
    "        for i in range(HIDDEN_LAYER_SIZE):\n",
    "            for j in range(HIDDEN_LAYER_SIZE):\n",
    "                weights_hidden1_to_hidden2[j][i] -= learning_rate * gradients_hidden1_to_hidden2[j][i] / minibatch_size\n",
    "\n",
    "        for i in range(INPUT_LAYER_SIZE):\n",
    "            for j in range(HIDDEN_LAYER_SIZE):\n",
    "                weights_input_to_hidden1[j][i] -= learning_rate * gradients_input_to_hidden1[j][i] / minibatch_size\n",
    "\n",
    "        for i in range(len(biases_output)):\n",
    "            biases_output[i] -= learning_rate * final_output_deltas[i] / minibatch_size\n",
    "\n",
    "        for i in range(len(biases_hidden2)):\n",
    "            biases_hidden2[i] -= learning_rate * final_hidden2_deltas[i] / minibatch_size\n",
    "\n",
    "        for i in range(len(biases_hidden1)):\n",
    "            biases_hidden1[i] -= learning_rate * final_hidden1_deltas[i] / minibatch_size\n",
    "\n",
    "    weights_biases = {\n",
    "        'weights_input_to_hidden1': weights_input_to_hidden1,\n",
    "        'biases_hidden1': biases_hidden1,\n",
    "        'weights_hidden1_to_hidden2': weights_hidden1_to_hidden2,\n",
    "        'biases_hidden2': biases_hidden2,\n",
    "        'weights_hidden2_to_output': weights_hidden2_to_output,\n",
    "        'biases_output': biases_output\n",
    "    }\n",
    "\n",
    "    with open('nn/weights_biases.pkl', 'wb') as f:\n",
    "        pickle.dump(weights_biases, f)\n",
    "\n",
    "    print(\"SGD minibatch Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GD(training_images_set, training_labels_set, weights, biases, learning_rate=0.05, epochs=100):\n",
    "    #SGD-mini but the minibatch is the entire dataset\n",
    "\n",
    "    weights_input_to_hidden1 = weights[0]\n",
    "    weights_hidden1_to_hidden2 = weights[1]\n",
    "    weights_hidden2_to_output = weights[2]\n",
    "\n",
    "    biases_hidden1 = biases[0]\n",
    "    biases_hidden2 = biases[1]\n",
    "    biases_output = biases[2]\n",
    "\n",
    "    all_images = len(training_images_set)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        new_weights = [weights_input_to_hidden1, weights_hidden1_to_hidden2, weights_hidden2_to_output]\n",
    "        new_biases = [biases_hidden1, biases_hidden2, biases_output]\n",
    "\n",
    "        if epoch % 10 == 0 and epoch != 0:\n",
    "            print(\"Accuracy after\", epoch, \"GD epochs:\", test_neural_network(TESTING_IMAGES, TESTING_LABELS, new_weights, new_biases))\n",
    "\n",
    "        # Initialize accumulators for gradients and deltas\n",
    "        final_output_deltas = [0] * OUTPUT_LAYER_SIZE\n",
    "        final_hidden2_deltas = [0] * HIDDEN_LAYER_SIZE\n",
    "        final_hidden1_deltas = [0] * HIDDEN_LAYER_SIZE\n",
    "\n",
    "        gradients_hidden2_to_output = [[0 for _ in range(HIDDEN_LAYER_SIZE)] for _ in range(OUTPUT_LAYER_SIZE)]\n",
    "        gradients_hidden1_to_hidden2 = [[0 for _ in range(HIDDEN_LAYER_SIZE)] for _ in range(HIDDEN_LAYER_SIZE)]\n",
    "        gradients_input_to_hidden1 = [[0 for _ in range(INPUT_LAYER_SIZE)] for _ in range(HIDDEN_LAYER_SIZE)]\n",
    "\n",
    "        for i in range(all_images):\n",
    "            input_layer = training_images_set[i]\n",
    "            label = training_labels_set[i]\n",
    "            neuron_activations = forward_pass(input_layer, new_weights, new_biases)\n",
    "\n",
    "            hidden_layer_1 = neuron_activations[0]\n",
    "            hidden_layer_2 = neuron_activations[1]\n",
    "            output_layer = softmax(neuron_activations[2])\n",
    "\n",
    "            actual_output = [0] * len(output_layer)\n",
    "            actual_output[label] = 1\n",
    "\n",
    "            output_layer_deltas = [output_layer[i] - actual_output[i] for i in range(len(output_layer))]\n",
    "            \n",
    "            #hidden2 to output gradients\n",
    "            for j in range(len(output_layer)):\n",
    "                for i in range(len(hidden_layer_2)):\n",
    "                    gradient = output_layer_deltas[j] * hidden_layer_2[i]\n",
    "                    gradients_hidden2_to_output[j][i] += gradient\n",
    "\n",
    "\n",
    "            #hidden1 to hidden2 output gradients\n",
    "            hidden_layer_2_deltas = [0] * HIDDEN_LAYER_SIZE\n",
    "            for j in range(len(hidden_layer_2)):\n",
    "                delta = sum(weights_hidden2_to_output[k][j] * output_layer_deltas[k] for k in range(OUTPUT_LAYER_SIZE))\n",
    "                delta *= relu_derivative(hidden_layer_2[j])\n",
    "                hidden_layer_2_deltas[j] = delta\n",
    "\n",
    "            for i in range(len(hidden_layer_1)):\n",
    "                for j in range(len(hidden_layer_2)):\n",
    "                    gradient = hidden_layer_2_deltas[j] * hidden_layer_1[i]\n",
    "                    gradients_hidden1_to_hidden2[j][i] += gradient\n",
    "\n",
    "            #input to hidden1 gradients\n",
    "            hidden_layer_1_deltas = [0] * HIDDEN_LAYER_SIZE\n",
    "            for j in range(len(hidden_layer_1)):\n",
    "                delta = sum(weights_hidden1_to_hidden2[k][j] * hidden_layer_2_deltas[k] for k in range(HIDDEN_LAYER_SIZE))\n",
    "                delta *= relu_derivative(hidden_layer_1[j])\n",
    "                hidden_layer_1_deltas[j] = delta\n",
    "\n",
    "            for i in range(len(input_layer)):\n",
    "                for j in range(len(hidden_layer_1)):\n",
    "                    gradient = hidden_layer_1_deltas[j] * input_layer[i]\n",
    "                    gradients_input_to_hidden1[j][i] += gradient\n",
    "\n",
    "            for i in range(OUTPUT_LAYER_SIZE):\n",
    "                final_output_deltas[i] += output_layer_deltas[i]\n",
    "\n",
    "            for i in range(HIDDEN_LAYER_SIZE):\n",
    "                final_hidden2_deltas[i] += hidden_layer_2_deltas[i]\n",
    "\n",
    "            for i in range(HIDDEN_LAYER_SIZE):\n",
    "                final_hidden1_deltas[i] += hidden_layer_1_deltas[i]\n",
    "\n",
    "        # Update weights and biases with averaged gradients\n",
    "\n",
    "        for i in range(HIDDEN_LAYER_SIZE):\n",
    "            for j in range(OUTPUT_LAYER_SIZE):\n",
    "                weights_hidden2_to_output[j][i] -= learning_rate * gradients_hidden2_to_output[j][i] / all_images\n",
    "\n",
    "        for i in range(HIDDEN_LAYER_SIZE):\n",
    "            for j in range(HIDDEN_LAYER_SIZE):\n",
    "                weights_hidden1_to_hidden2[j][i] -= learning_rate * gradients_hidden1_to_hidden2[j][i] / all_images\n",
    "\n",
    "        for i in range(INPUT_LAYER_SIZE):\n",
    "            for j in range(HIDDEN_LAYER_SIZE):\n",
    "                weights_input_to_hidden1[j][i] -= learning_rate * gradients_input_to_hidden1[j][i] / all_images\n",
    "                \n",
    "        for i in range(len(biases_output)):\n",
    "            biases_output[i] -= learning_rate * final_output_deltas[i] / all_images\n",
    "\n",
    "        for i in range(len(biases_hidden2)):\n",
    "            biases_hidden2[i] -= learning_rate * final_hidden2_deltas[i] / all_images\n",
    "\n",
    "        for i in range(len(biases_hidden1)):\n",
    "            biases_hidden1[i] -= learning_rate * final_hidden1_deltas[i] / all_images\n",
    "\n",
    "    weights_biases = {\n",
    "        'weights_input_to_hidden1': weights_input_to_hidden1,\n",
    "        'biases_hidden1': biases_hidden1,\n",
    "        'weights_hidden1_to_hidden2': weights_hidden1_to_hidden2,\n",
    "        'biases_hidden2': biases_hidden2,\n",
    "        'weights_hidden2_to_output': weights_hidden2_to_output,\n",
    "        'biases_output': biases_output\n",
    "    }\n",
    "\n",
    "    with open('nn/weights_biases.pkl', 'wb') as f:\n",
    "        pickle.dump(weights_biases, f)\n",
    "\n",
    "    print(\"GD Completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def he_initialization(fan_in, fan_out):\n",
    "    std = math.sqrt(2.0 / fan_in)\n",
    "    return [[random.gauss(0, std) for _ in range(fan_in)] for _ in range(fan_out)]\n",
    "\n",
    "#to get a fresh set of weights and biases\n",
    "weights_biases = {\n",
    "    'weights_input_to_hidden1': he_initialization(INPUT_LAYER_SIZE, HIDDEN_LAYER_SIZE),\n",
    "    'biases_hidden1': [0.0 for _ in range(HIDDEN_LAYER_SIZE)],\n",
    "    'weights_hidden1_to_hidden2': he_initialization(HIDDEN_LAYER_SIZE, HIDDEN_LAYER_SIZE),\n",
    "    'biases_hidden2': [0.0 for _ in range(HIDDEN_LAYER_SIZE)],\n",
    "    'weights_hidden2_to_output': he_initialization(HIDDEN_LAYER_SIZE, OUTPUT_LAYER_SIZE),\n",
    "    'biases_output': [0.0 for _ in range(OUTPUT_LAYER_SIZE)]\n",
    "}\n",
    "\n",
    "#now that we have initialised them, save it with pickle\n",
    "with open('nn/weights_biases.pkl', 'wb') as f:\n",
    "    pickle.dump(weights_biases, f)\n",
    "\n",
    "weights, biases = unpack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the optimisation method\n",
    "SGD(TRAINING_IMAGES, TRAINING_LABELS, weights, biases)\n",
    "#SGD_minibatch(TRAINING_IMAGES, TRAINING_LABELS, weights, biases)\n",
    "#GD(TRAINING_IMAGES, TRAINING_LABELS, weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases = unpack()\n",
    "test_neural_network(TESTING_IMAGES, TESTING_LABELS, weights, biases)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
